{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec: Training & Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import glob\n",
    "import logging\n",
    "import spacy\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_path = \"corpus/corpus_harrypotter/annotated.txt\"\n",
    "model_path = \"results/model_harrypotter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = LineSentence(corpus_path)\n",
    "\n",
    "# remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in corpus:\n",
    "     for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "corpus = [[token for token in text if frequency[token] > 1]\n",
    "         for text in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harry|PERSON potter|PERSON and|CONJ the|DET sorcerer|NORP stone|PERSON chapter|NOUN one|CARDINAL the|DET boy|NOUN who|NOUN lived|VERB mr.|PROPN and|CONJ mrs.|PROPN dursley|PERSON of|ADP number|NOUN four|CARDINAL privet|PROPN drive|PROPN were|VERB proud|ADJ say|VERB that|ADP they|PRON were|VERB perfectly|ADV normal|ADJ thank|VERB you|PRON very|ADV much|ADV ... all|DET was|VERB well|ADV\n"
     ]
    }
   ],
   "source": [
    "print (\" \".join(corpus[0] + [\"...\"] + corpus[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-12 13:39:48,448 : INFO : collecting all words and their counts\n",
      "2017-01-12 13:39:48,450 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-01-12 13:39:48,505 : INFO : PROGRESS: at sentence #10000, processed 126257 words, keeping 8938 word types\n",
      "2017-01-12 13:39:48,546 : INFO : PROGRESS: at sentence #20000, processed 250501 words, keeping 12149 word types\n",
      "2017-01-12 13:39:48,592 : INFO : PROGRESS: at sentence #30000, processed 380310 words, keeping 14518 word types\n",
      "2017-01-12 13:39:48,639 : INFO : PROGRESS: at sentence #40000, processed 502565 words, keeping 16269 word types\n",
      "2017-01-12 13:39:48,686 : INFO : PROGRESS: at sentence #50000, processed 634029 words, keeping 17691 word types\n",
      "2017-01-12 13:39:48,732 : INFO : PROGRESS: at sentence #60000, processed 764250 words, keeping 18639 word types\n",
      "2017-01-12 13:39:48,781 : INFO : PROGRESS: at sentence #70000, processed 894499 words, keeping 19341 word types\n",
      "2017-01-12 13:39:48,826 : INFO : PROGRESS: at sentence #80000, processed 1018785 words, keeping 19760 word types\n",
      "2017-01-12 13:39:48,848 : INFO : collected 19804 word types from a corpus of 1067788 raw words and 84062 sentences\n",
      "2017-01-12 13:39:48,850 : INFO : Loading a fresh vocabulary\n",
      "2017-01-12 13:39:49,085 : INFO : min_count=1 retains 19804 unique words (100% of original 19804, drops 0)\n",
      "2017-01-12 13:39:49,087 : INFO : min_count=1 leaves 1067788 word corpus (100% of original 1067788, drops 0)\n",
      "2017-01-12 13:39:49,216 : INFO : deleting the raw counts dictionary of 19804 items\n",
      "2017-01-12 13:39:49,220 : INFO : sample=0.001 downsamples 49 most-common words\n",
      "2017-01-12 13:39:49,223 : INFO : downsampling leaves estimated 825745 word corpus (77.3% of prior 1067788)\n",
      "2017-01-12 13:39:49,225 : INFO : estimated required memory for 19804 words and 300 dimensions: 57431600 bytes\n",
      "2017-01-12 13:39:49,428 : INFO : resetting layer weights\n",
      "2017-01-12 13:39:49,883 : INFO : training model with 2 workers on 19804 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=2 window=5\n",
      "2017-01-12 13:39:49,884 : INFO : expecting 84062 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-01-12 13:39:50,910 : INFO : PROGRESS: at 2.58% examples, 640524 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:39:51,915 : INFO : PROGRESS: at 5.28% examples, 650381 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:39:52,919 : INFO : PROGRESS: at 7.96% examples, 654072 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:39:53,924 : INFO : PROGRESS: at 10.61% examples, 653917 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:39:54,930 : INFO : PROGRESS: at 13.32% examples, 656694 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:39:55,945 : INFO : PROGRESS: at 15.99% examples, 656553 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:39:56,947 : INFO : PROGRESS: at 18.68% examples, 657441 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:39:57,958 : INFO : PROGRESS: at 21.37% examples, 657268 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:39:58,965 : INFO : PROGRESS: at 24.05% examples, 657541 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:39:59,970 : INFO : PROGRESS: at 26.76% examples, 658684 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:40:00,976 : INFO : PROGRESS: at 29.39% examples, 658284 words/s, in_qsize 4, out_qsize 0\n",
      "2017-01-12 13:40:01,979 : INFO : PROGRESS: at 32.09% examples, 658614 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:40:02,984 : INFO : PROGRESS: at 34.86% examples, 659870 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:40:03,987 : INFO : PROGRESS: at 37.33% examples, 656761 words/s, in_qsize 4, out_qsize 0\n",
      "2017-01-12 13:40:04,998 : INFO : PROGRESS: at 39.95% examples, 655800 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:40:05,999 : INFO : PROGRESS: at 42.59% examples, 655872 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:40:07,004 : INFO : PROGRESS: at 44.96% examples, 651117 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:40:08,009 : INFO : PROGRESS: at 47.48% examples, 649559 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:40:09,013 : INFO : PROGRESS: at 50.12% examples, 649775 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:40:10,025 : INFO : PROGRESS: at 52.78% examples, 650158 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:40:11,030 : INFO : PROGRESS: at 55.50% examples, 651073 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:40:12,035 : INFO : PROGRESS: at 58.08% examples, 650053 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:40:13,040 : INFO : PROGRESS: at 60.66% examples, 649553 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:40:14,047 : INFO : PROGRESS: at 63.31% examples, 649668 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:40:15,049 : INFO : PROGRESS: at 66.01% examples, 650570 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:40:16,060 : INFO : PROGRESS: at 68.73% examples, 651101 words/s, in_qsize 4, out_qsize 0\n",
      "2017-01-12 13:40:17,069 : INFO : PROGRESS: at 71.43% examples, 651337 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:40:18,069 : INFO : PROGRESS: at 73.94% examples, 650441 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:40:19,073 : INFO : PROGRESS: at 76.69% examples, 651351 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:40:20,078 : INFO : PROGRESS: at 79.41% examples, 652233 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:40:21,079 : INFO : PROGRESS: at 82.17% examples, 653105 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:40:22,092 : INFO : PROGRESS: at 84.94% examples, 653653 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:40:23,100 : INFO : PROGRESS: at 87.67% examples, 654274 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:40:24,100 : INFO : PROGRESS: at 90.41% examples, 655014 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:40:25,103 : INFO : PROGRESS: at 93.18% examples, 655895 words/s, in_qsize 4, out_qsize 0\n",
      "2017-01-12 13:40:26,111 : INFO : PROGRESS: at 95.89% examples, 656226 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:40:27,115 : INFO : PROGRESS: at 98.53% examples, 655962 words/s, in_qsize 3, out_qsize 0\n",
      "2017-01-12 13:40:27,645 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-01-12 13:40:27,653 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-01-12 13:40:27,654 : INFO : training on 32033640 raw words (24772622 effective words) took 37.7s, 656299 effective words/s\n",
      "2017-01-12 13:40:27,696 : INFO : saving Word2Vec object under results/model_harrypotter, separately None\n",
      "2017-01-12 13:40:27,697 : INFO : not storing attribute cum_table\n",
      "2017-01-12 13:40:27,699 : INFO : not storing attribute syn0norm\n",
      "2017-01-12 13:40:28,763 : INFO : saved results/model_harrypotter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model in 40.32 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Now train a model!\n",
    "\n",
    "# Parameters (see http://radimrehurek.com/gensim/models/word2vec.html):\n",
    "\n",
    "# Degree of parallelization. Requires cython installed.\n",
    "workers = 2\n",
    "\n",
    "# Size of the neural network layer that corresponds\n",
    "# to the size of the output vector. A bigger size\n",
    "# requires more training data, but can lead to more\n",
    "# accurate results.\n",
    "size = 300\n",
    "\n",
    "# How often a token must appear in the corpus.\n",
    "min_count = 1\n",
    "\n",
    "# The size of the window. It is the maximum distance\n",
    "# between the current and predicted word within a sentence.\n",
    "window = 5\n",
    "\n",
    "# Epochs. How often do we iterate over the corpus?\n",
    "# If you really want to train something, use 15-20 epochs\n",
    "epochs = 30\n",
    "\n",
    "# Defines the training algorithm. By default (sg=0), \n",
    "# CBOW is used. Otherwise (sg=1), skip-gram is employed.\n",
    "skip_gram = 0\n",
    "\n",
    "# If > 0, negative sampling will be used, the int for \n",
    "# negative specifies how many “noise words” should be drawn\n",
    "# (usually between 5-20). Default is 5.\n",
    "# If set to 0, no negative samping is used.\n",
    "negative = 2\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=corpus,\n",
    "    workers=workers,\n",
    "    size=size,\n",
    "    min_count=min_count,\n",
    "    window=window,\n",
    "    iter=epochs,\n",
    "    sg=skip_gram,\n",
    "    negative=negative\n",
    ")\n",
    "\n",
    "model.save(model_path)\n",
    "\n",
    "print(\"Trained model in %0.2f seconds.\" % (time.time() - t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tag_word(word):\n",
    "    for sentence in nlp(word).sents:\n",
    "        for token in sentence:\n",
    "            tag = token.ent_type_ or token.pos_\n",
    "            return \"{}|{}\".format(token.text.lower(), tag)\n",
    "\n",
    "def get_sim(word, tag):\n",
    "    encoded = tag_word(word)\n",
    "    for sim_word, sim_score in model.most_similar(positive=[encoded], topn=1000):\n",
    "        if tag in sim_word:\n",
    "            yield sim_word.split('|')[0], sim_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry pulse merely with neville.\n",
      "Hermione dreams hastily with ginny.\n",
      "Ron gasp hastily with ginny.\n",
      "Malfoy deranged berserk with goyle.\n",
      "Snape retiring imploringly with umbridge.\n",
      "Slughorn butt imploringly with umbridge.\n",
      "Ginny crooning currently with wit.\n"
     ]
    }
   ],
   "source": [
    "for name in [\"Harry\", \"Hermione\", \"Ron\", \"Malfoy\", \"Snape\", \"Slughorn\", \"Ginny\"]:\n",
    "    print(\"{name} {verb} {adv} with {person}.\".format(\n",
    "        name=name,\n",
    "        verb=list(get_sim(name, \"VERB\"))[0][0],\n",
    "        adv=list(get_sim(name, \"ADV\"))[0][0],\n",
    "        person=list(get_sim(name, \"PERSON\"))[0][0],\n",
    "        adj=list(get_sim(name, \"ADJ\"))[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characters Compared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "happy\n",
      "1. Snape 0.07\n",
      "2. Hermione 0.05\n",
      "3. Harry 0.01\n",
      "4. Ron -0.02\n",
      "5. Malfoy -0.04\n",
      "\n",
      "nice\n",
      "1. Hermione 0.01\n",
      "2. Ron -0.06\n",
      "3. Snape -0.07\n",
      "4. Malfoy -0.10\n",
      "5. Harry -0.18\n",
      "\n",
      "evil\n",
      "1. Snape 0.05\n",
      "2. Hermione 0.04\n",
      "3. Malfoy 0.03\n",
      "4. Ron -0.01\n",
      "5. Harry -0.05\n",
      "\n",
      "tearful\n",
      "1. Malfoy 0.05\n",
      "2. Snape 0.04\n",
      "3. Ron 0.02\n",
      "4. Hermione -0.01\n",
      "5. Harry -0.01\n"
     ]
    }
   ],
   "source": [
    "characters = [\"Harry\", \"Hermione\", \"Ron\", \"Malfoy\", \"Snape\"]\n",
    "attributes = [\"happy\", \"nice\", \"evil\", \"tearful\"]\n",
    "\n",
    "for attribute in attributes:\n",
    "    scores = {character: model.similarity(tag_word(character), tag_word(attribute))\n",
    "              for character in characters}\n",
    "    print()\n",
    "    print(attribute)\n",
    "    for i, (character, score) in enumerate(\n",
    "            sorted(scores.items(), key=lambda x: x[1], reverse=True)):\n",
    "        print(\"{}. {} {:.2f}\".format(i+1, character, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
