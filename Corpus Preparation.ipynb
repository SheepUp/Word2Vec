{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import glob\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_path = \"corpus/corpus_lotr/\"\n",
    "# corpus_path = \"corpus/corpus_harrypotter/\"\n",
    "# corpus_path = \"corpus/corpus_pride_and_prejudice/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load spaCy models.\n",
    "# This takes a while and requires a few gig RAM. \n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_iterator(path):\n",
    "    for f in glob.glob(\"%s/*.txt\" % path):\n",
    "        if \"annotated\" not in f:\n",
    "            yield \" \".join(list(codecs.open(f, \"r\", encoding='utf-8', errors='ignore')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all the text\n",
    "text = list(text_iterator(corpus_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Analyze!\n",
    "ignore = {\"PUNCT\", \"SPACE\", \"PART\"}\n",
    "corpus = []\n",
    "for doc in nlp.pipe(text, batch_size=2, n_threads=4):\n",
    "    for sentence in doc.sents:\n",
    "        sent = []\n",
    "        for word in sentence:\n",
    "            if word.pos_ not in ignore:\n",
    "                tag = word.ent_type_ or word.pos_\n",
    "#                 sent.append(\"{}|{}\".format(word.text.lower(), tag))\n",
    "                sent.append(\"{}\".format(word.text.lower()))\n",
    "        if sent:\n",
    "            corpus.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"{}prepared.txt\".format(corpus_path), 'w') as file_:\n",
    "    for sentence in corpus:\n",
    "        if sentence:\n",
    "            file_.write(\"{}\\n\".format(\" \".join(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33355 sentences in corpus.\n"
     ]
    }
   ],
   "source": [
    "print(\"%s sentences in corpus.\" % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
